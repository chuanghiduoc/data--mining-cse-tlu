# ğŸ”¬ THá»°C NGHIá»†M SO SÃNH RANDOM FOREST VÃ€ NAIVE BAYES (NÃ‚NG CAO)

## ğŸ“‹ MÃ´ táº£ dá»± Ã¡n

Thá»±c nghiá»‡m **nÃ¢ng cao** so sÃ¡nh hiá»‡u quáº£ cá»§a thuáº­t toÃ¡n **Random Forest** vÃ  **Naive Bayes** trÃªn **7 datasets** khÃ¡c nhau vá»›i **nhiá»u ká»¹ thuáº­t tiá»n xá»­ lÃ½ vÃ  khai thÃ¡c dá»¯ liá»‡u**:

### ğŸ“Š Datasets Ä‘Æ°á»£c sá»­ dá»¥ng:

1. **SMS Spam** - Text classification (5,572 samples)
2. **Wine Quality** - Numeric features (1,599 samples)
3. **Diabetes** - Medical data vá»›i missing values (768 samples)
4. **Adult Census** - Mixed data types (32,561 samples)
5. **Mushroom** - Categorical features (8,124 samples)
6. **Sonar** - High-dimensional numeric (208 samples, 60 features)
7. **Credit Card Fraud** - Large imbalanced dataset (284,807 samples)

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
btl/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ spam.csv              # SMS Spam dataset
â”‚   â”œâ”€â”€ winequality-red.csv   # Wine Quality dataset
â”‚   â”œâ”€â”€ diabetes.csv          # Pima Indians Diabetes
â”‚   â”œâ”€â”€ adult.csv             # Adult Census Income
â”‚   â”œâ”€â”€ mushrooms.csv         # Mushroom Classification
â”‚   â”œâ”€â”€ sonar-all-data.csv    # Sonar Mines vs Rocks
â”‚   â””â”€â”€ creditcard.csv        # Credit Card Fraud Detection
â”œâ”€â”€ random_forest_experiment.py     # Code thá»±c nghiá»‡m nÃ¢ng cao
â”œâ”€â”€ requirements.txt                # ThÆ° viá»‡n cáº§n thiáº¿t
â”œâ”€â”€ TECHNIQUES_GUIDE.md             # HÆ°á»›ng dáº«n chi tiáº¿t cÃ¡c ká»¹ thuáº­t
â”œâ”€â”€ setup_and_run.bat               # Script tá»± Ä‘á»™ng cÃ i Ä‘áº·t vÃ  cháº¡y
â”œâ”€â”€ Random_Forest_Theory_Report.md  # BÃ¡o cÃ¡o lÃ½ thuyáº¿t
â””â”€â”€ README.md                       # HÆ°á»›ng dáº«n nÃ y
```

## ğŸš€ CÃ¡ch cháº¡y thá»±c nghiá»‡m

### PhÆ°Æ¡ng phÃ¡p 1: Tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)
```bash
setup_and_run.bat
```
Script nÃ y sáº½ tá»± Ä‘á»™ng:
1. CÃ i Ä‘áº·t táº¥t cáº£ dependencies
2. Kiá»ƒm tra datasets
3. Cháº¡y thá»±c nghiá»‡m

### PhÆ°Æ¡ng phÃ¡p 2: Thá»§ cÃ´ng

#### BÆ°á»›c 1: CÃ i Ä‘áº·t thÆ° viá»‡n
```bash
pip install -r requirements.txt
```

#### BÆ°á»›c 2: Cháº¡y thá»±c nghiá»‡m
```bash
python random_forest_experiment.py
```

## ğŸ“Š Káº¿t quáº£ sáº½ Ä‘Æ°á»£c táº¡o ra

1. **eda_analysis_all.png** - KhÃ¡m phÃ¡ dá»¯ liá»‡u cho táº¥t cáº£ datasets
2. **overall_comparison.png** - So sÃ¡nh tá»•ng thá»ƒ RF vs NB
3. **confusion_matrices_all.png** - Confusion matrices cho tá»«ng dataset

## ğŸ”¬ Quy trÃ¬nh khai phÃ¡ dá»¯ liá»‡u (Data Mining Process)

### 1. Thu tháº­p dá»¯ liá»‡u
- Táº£i 7 datasets tá»« thÆ° má»¥c `datasets/`
- Kiá»ƒm tra kÃ­ch thÆ°á»›c, kiá»ƒu dá»¯ liá»‡u

### 2. KhÃ¡m phÃ¡ dá»¯ liá»‡u (EDA)
- PhÃ¢n tÃ­ch phÃ¢n bá»‘ target
- PhÃ¡t hiá»‡n missing values
- PhÃ¢n loáº¡i feature types (numeric/categorical)
- Visualizations

### 3. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (NÃ‚NG CAO)

#### Techniques Ä‘Æ°á»£c Ã¡p dá»¥ng:
- **Text Processing**: TF-IDF vá»›i n-grams, Feature Selection (Chi-squared)
- **Missing Values**: Smart imputation (median/mode), xá»­ lÃ½ medical impossibility
- **Scaling**: StandardScaler, RobustScaler (robust to outliers)
- **Feature Engineering**: Interaction features, domain-specific features
- **Imbalanced Data**: SMOTE (oversampling), Random Undersampling
- **Dimensionality Reduction**: PCA (giáº£m tá»« 60â†’30 features cho Sonar)
- **Categorical Encoding**: Label Encoding, One-Hot Encoding
- **Feature Selection**: Chi2, Mutual Information, SelectKBest

#### Chi tiáº¿t cho tá»«ng dataset:

| Dataset | Techniques |
|---------|-----------|
| **SMS** | TF-IDF â†’ Chi2 Feature Selection (3000â†’500) |
| **Wine** | RobustScaler â†’ Feature Engineering (interaction) |
| **Diabetes** | Missing imputation â†’ StandardScaler â†’ SMOTE |
| **Adult** | Mixed encoding â†’ Imputation â†’ StandardScaler |
| **Mushroom** | Label Encoding â†’ Mutual Info Selection |
| **Sonar** | StandardScaler â†’ PCA (60â†’30 features) |
| **CreditCard** | StandardScaler â†’ Random Undersampling |

### 4. XÃ¢y dá»±ng mÃ´ hÃ¬nh
- **Random Forest**: 100 trees, max_depth=15, hyperparameter tuned
- **Naive Bayes**: 
  - MultinomialNB (text)
  - GaussianNB (numeric)
  - BernoulliNB (binary)

### 5. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
- **Metrics**: Accuracy, Precision, Recall, F1-score, AUC-ROC
- **Visualizations**: Confusion Matrix, ROC Curves
- **Validation**: Cross-validation (5-fold)

### 6. Trá»±c quan hÃ³a káº¿t quáº£
- So sÃ¡nh performance trÃªn tá»«ng dataset
- Average metrics across all datasets
- Win/Loss analysis
- Feature importance

### 7. BÃ¡o cÃ¡o káº¿t quáº£
- Best performers cho tá»«ng dataset
- Káº¿t luáº­n vÃ  khuyáº¿n nghá»‹
- Techniques summary

## ğŸ“ˆ Káº¿t quáº£ dá»± kiáº¿n

| Dataset | Features | Random Forest | Naive Bayes | Tá»‘t hÆ¡n |
|---------|----------|---------------|-------------|---------|
| **SMS** | 500 (text) | ~95% | **~97%** | NB |
| **Wine** | 13 (numeric) | **~85%** | ~78% | RF |
| **Diabetes** | 8 (numeric) | **~78%** | ~75% | RF |
| **Adult** | 14 (mixed) | **~86%** | ~82% | RF |
| **Mushroom** | 15 (categorical) | **~100%** | ~97% | RF |
| **Sonar** | 30 (PCA) | **~82%** | ~75% | RF |
| **CreditCard** | 30 (numeric) | **~95%** | ~88% | RF |

## ğŸ’¡ Káº¿t luáº­n chi tiáº¿t

### âœ… Random Forest tá»‘t hÆ¡n khi:
- Dá»¯ liá»‡u numeric vá»›i relationships phá»©c táº¡p
- Mixed data types
- Cáº§n xá»­ lÃ½ outliers vÃ  missing values
- Features cÃ³ correlation cao
- Cáº§n feature importance
- Dataset lá»›n

### âœ… Naive Bayes tá»‘t hÆ¡n khi:
- Text classification (TF-IDF)
- Categorical data vá»›i features Ä‘á»™c láº­p
- Cáº§n tá»‘c Ä‘á»™ training nhanh
- Dataset nhá»
- Real-time prediction
- Interpretability quan trá»ng

## ğŸ› ï¸ YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8 trá»Ÿ lÃªn
- **RAM**: Tá»‘i thiá»ƒu 8GB (vÃ¬ Credit Card dataset lá»›n)
- **Disk**: ~500MB cho datasets vÃ  káº¿t quáº£
- **OS**: Windows/Linux/MacOS

## ğŸ“¦ Dependencies

```
pandas>=1.5.0
numpy>=1.21.0
scikit-learn>=1.2.0
matplotlib>=3.5.0
seaborn>=0.12.0
imbalanced-learn>=0.10.0
scipy>=1.9.0
```

## ğŸ“ CÃ¡c ká»¹ thuáº­t Ä‘Ã£ Ã¡p dá»¥ng

### Tiá»n xá»­ lÃ½ (Preprocessing):
âœ… TF-IDF Vectorization  
âœ… Feature Selection (Chi2, Mutual Information)  
âœ… Missing Value Imputation  
âœ… Feature Scaling (Standard, Robust, MinMax)  
âœ… Feature Engineering  
âœ… Categorical Encoding  

### Xá»­ lÃ½ dá»¯ liá»‡u (Data Handling):
âœ… Imbalanced Data (SMOTE, Undersampling)  
âœ… Dimensionality Reduction (PCA)  
âœ… Cross-validation  
âœ… Stratified splitting  

### ÄÃ¡nh giÃ¡ (Evaluation):
âœ… Multiple metrics (Accuracy, Precision, Recall, F1, AUC)  
âœ… Confusion Matrix  
âœ… ROC Curves  
âœ… Feature Importance  

## ğŸ“š TÃ i liá»‡u tham kháº£o

- **`TECHNIQUES_GUIDE.md`**: HÆ°á»›ng dáº«n chi tiáº¿t vá» tá»«ng ká»¹ thuáº­t
- **`Random_Forest_Theory_Report.md`**: LÃ½ thuyáº¿t vá» Random Forest
- Code cÃ³ comments Ä‘áº§y Ä‘á»§

## â“ Troubleshooting

### Lá»—i: "Module 'imblearn' not found"
```bash
pip install imbalanced-learn
```

### Lá»—i: "Memory Error" vá»›i Credit Card dataset
â†’ Giáº£m `sampling_strategy` trong code hoáº·c skip dataset nÃ y

### Warning vá» sklearn features
â†’ CÃ³ thá»ƒ ignore, khÃ´ng áº£nh hÆ°á»Ÿng káº¿t quáº£

## ğŸ“§ LiÃªn há»‡ & Há»— trá»£

Náº¿u cÃ³ váº¥n Ä‘á» trong quÃ¡ trÃ¬nh cháº¡y thá»±c nghiá»‡m, vui lÃ²ng kiá»ƒm tra:

1. âœ… ÄÃ£ cÃ i Ä‘áº·t Ä‘Ãºng thÆ° viá»‡n chÆ°a (`pip list`)
2. âœ… Datasets cÃ³ trong thÆ° má»¥c `datasets/` chÆ°a
3. âœ… Python version >= 3.8 (`python --version`)
4. âœ… Äá»§ RAM (8GB recommended)

### Quick Test:
```bash
python -c "import pandas, numpy, sklearn, imblearn; print('All dependencies OK!')"
```

## ğŸ¯ Má»¥c tiÃªu há»c táº­p

Thá»±c nghiá»‡m nÃ y giÃºp báº¡n:
- âœ¨ Hiá»ƒu rÃµ Random Forest vs Naive Bayes
- âœ¨ Ãp dá»¥ng nhiá»u ká»¹ thuáº­t tiá»n xá»­ lÃ½ nÃ¢ng cao
- âœ¨ Xá»­ lÃ½ cÃ¡c loáº¡i dá»¯ liá»‡u khÃ¡c nhau (text, numeric, mixed, categorical)
- âœ¨ Giáº£i quyáº¿t váº¥n Ä‘á» imbalanced data
- âœ¨ Ãp dá»¥ng dimensionality reduction
- âœ¨ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh má»™t cÃ¡ch toÃ n diá»‡n

---

**ğŸ“Œ Note**: Dataset Credit Card (~300MB) cÃ³ thá»ƒ máº¥t vÃ i phÃºt Ä‘á»ƒ xá»­ lÃ½. Báº¡n cÃ³ thá»ƒ comment out trong code náº¿u muá»‘n cháº¡y nhanh hÆ¡n.

*Thá»±c nghiá»‡m nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ minh há»a toÃ n diá»‡n cÃ¡c ká»¹ thuáº­t khai phÃ¡ dá»¯ liá»‡u vÃ  so sÃ¡nh hiá»‡u suáº¥t Random Forest vs Naive Bayes trÃªn nhiá»u loáº¡i dá»¯ liá»‡u khÃ¡c nhau.*
