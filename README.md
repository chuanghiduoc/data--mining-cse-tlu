# ğŸ”¬ So sÃ¡nh Random Forest vs Naive Bayes - Khai phÃ¡ dá»¯ liá»‡u nÃ¢ng cao

## ğŸ“‹ MÃ´ táº£ dá»± Ã¡n

Thá»±c nghiá»‡m **nÃ¢ng cao** so sÃ¡nh hiá»‡u quáº£ thuáº­t toÃ¡n **Random Forest** vÃ  **Naive Bayes** trÃªn **6 datasets khÃ¡c nhau** vá»›i nhiá»u ká»¹ thuáº­t tiá»n xá»­ lÃ½ vÃ  khai thÃ¡c dá»¯ liá»‡u.

### ğŸ¯ Má»¥c tiÃªu
- So sÃ¡nh hiá»‡u suáº¥t RF vs NB trÃªn nhiá»u loáº¡i dá»¯ liá»‡u
- Ãp dá»¥ng ká»¹ thuáº­t tiá»n xá»­ lÃ½ phÃ¹ há»£p cho tá»«ng dataset
- Xá»­ lÃ½ imbalanced data, missing values, high-dimensional data
- ÄÃ¡nh giÃ¡ toÃ n diá»‡n vá»›i nhiá»u metrics

---

## ğŸ“Š Datasets sá»­ dá»¥ng

| Dataset | Samples | Features | Loáº¡i dá»¯ liá»‡u | MÃ´ táº£ |
|---------|---------|----------|--------------|-------|
| **Wine Quality** | 1,599 | 12 | Numeric | PhÃ¢n loáº¡i cháº¥t lÆ°á»£ng rÆ°á»£u vang |
| **Diabetes** | 768 | 9 | Numeric | Dá»± Ä‘oÃ¡n bá»‡nh tiá»ƒu Ä‘Æ°á»ng |
| **Adult Census** | 32,561 | 15 | Mixed | Dá»± Ä‘oÃ¡n thu nháº­p >50K |
| **Mushroom** | 8,124 | 23 | Categorical | PhÃ¢n loáº¡i náº¥m Ä‘á»™c/Äƒn Ä‘Æ°á»£c |
| **Sonar** | 208 | 61 | Numeric | PhÃ¢n biá»‡t Ä‘Ã¡/mÃ¬n tá»« sonar |
| **Credit Card** | 284,807 | 31 | Numeric | PhÃ¡t hiá»‡n gian láº­n tháº» tÃ­n dá»¥ng |

### ğŸ“¥ Download Datasets

**Datasets KHÃ”NG Ä‘Æ°á»£c lÆ°u trÃªn GitHub** (do file quÃ¡ lá»›n). Vui lÃ²ng táº£i vá»:

1. **Wine Quality**: [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/wine+quality)
2. **Diabetes**: [Kaggle - Pima Indians Diabetes](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)
3. **Adult**: [Kaggle - Adult Census Income](https://www.kaggle.com/datasets/uciml/adult-census-income)
4. **Mushroom**: [Kaggle - Mushroom Classification](https://www.kaggle.com/datasets/uciml/mushroom-classification)
5. **Sonar**: [Kaggle - Sonar Mines vs Rocks](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks)
6. **Credit Card**: [Kaggle - Credit Card Fraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

**Sau khi táº£i vá», Ä‘áº·t vÃ o thÆ° má»¥c `datasets/` vá»›i tÃªn:**
```
datasets/
â”œâ”€â”€ winequality-red.csv
â”œâ”€â”€ diabetes.csv
â”œâ”€â”€ adult.csv
â”œâ”€â”€ mushrooms.csv
â”œâ”€â”€ sonar-all-data.csv
â””â”€â”€ creditcard.csv
```

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
btl/
â”œâ”€â”€ datasets/              # Datasets (táº£i vá» riÃªng)
â”œâ”€â”€ random_forest_experiment.py  # Code thá»±c nghiá»‡m chÃ­nh
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ README.md             # File nÃ y
â”œâ”€â”€ .gitignore            # Git ignore (datasets)
â”‚
â”œâ”€â”€ Random_Forest_Theory_Report.md   # BÃ¡o cÃ¡o lÃ½ thuyáº¿t
â”œâ”€â”€ Random_Forest_Giang_Bai.md      # Giáº£ng bÃ i
â”‚
â””â”€â”€ Output files (generated):
    â”œâ”€â”€ eda_analysis_all.png
    â”œâ”€â”€ overall_comparison.png
    â””â”€â”€ confusion_matrices_all.png
```

---

## ğŸš€ CÃ¡ch cháº¡y

### BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

**Dependencies cáº§n thiáº¿t:**
- pandas, numpy
- scikit-learn
- matplotlib, seaborn
- imbalanced-learn

### BÆ°á»›c 2: Download datasets

Táº£i 6 datasets tá»« links á»Ÿ trÃªn vÃ  Ä‘áº·t vÃ o `datasets/`

### BÆ°á»›c 3: Cháº¡y experiment

```bash
python random_forest_experiment.py
```

---

## ğŸ”¬ Quy trÃ¬nh thá»±c nghiá»‡m

### **BÆ°á»›c 1: Thu tháº­p dá»¯ liá»‡u**
Load 6 datasets vÃ  kiá»ƒm tra cáº¥u trÃºc

### **BÆ°á»›c 2: KhÃ¡m phÃ¡ dá»¯ liá»‡u (EDA)**
- PhÃ¢n tÃ­ch phÃ¢n bá»‘ target
- PhÃ¡t hiá»‡n missing values
- PhÃ¢n loáº¡i feature types
- Visualizations

### **BÆ°á»›c 3: Tiá»n xá»­ lÃ½ dá»¯ liá»‡u**

#### Ká»¹ thuáº­t Ã¡p dá»¥ng theo dataset:

| Dataset | Techniques |
|---------|-----------|
| **Wine** | RobustScaler â†’ Feature Engineering (interaction features) |
| **Diabetes** | Medical impossibility handling â†’ Median imputation â†’ StandardScaler â†’ **SMOTE** |
| **Adult** | Missing value handling â†’ Mixed encoding â†’ StandardScaler |
| **Mushroom** | Label Encoding â†’ **Mutual Information** feature selection (23â†’15 features) |
| **Sonar** | StandardScaler â†’ **PCA** (60â†’30 features, 95% variance) |
| **CreditCard** | StandardScaler â†’ **Random Undersampling** (284Kâ†’1.5K samples) |

### **BÆ°á»›c 3.5: Giáº£m chiá»u**
Ãp dá»¥ng **PCA** cho Wine, Adult, CreditCard (so sÃ¡nh trÆ°á»›c/sau PCA)

### **BÆ°á»›c 4: XÃ¢y dá»±ng mÃ´ hÃ¬nh**

#### Random Forest
```python
RandomForestClassifier(
    n_estimators=100,      # 100 trees
    max_depth=15,          # TrÃ¡nh overfitting
    min_samples_split=5,
    min_samples_leaf=2,
    n_jobs=-1              # Parallel processing
)
```

#### Naive Bayes
```python
GaussianNB()  # Cho numeric data
```

### **BÆ°á»›c 5: ÄÃ¡nh giÃ¡**

**Metrics sá»­ dá»¥ng:**
- âœ… Accuracy: Tá»•ng thá»ƒ Ä‘Ãºng bao nhiÃªu %
- âœ… Precision: False Positive cost
- âœ… Recall: False Negative cost  
- âœ… F1-Score: Harmonic mean (imbalanced data)
- âœ… AUC-ROC: Kháº£ nÄƒng phÃ¢n biá»‡t classes

### **BÆ°á»›c 6: Visualization**
- Overall comparison (accuracy, F1, win rate)
- Confusion matrices
- Performance comparison

### **BÆ°á»›c 7: BÃ¡o cÃ¡o**
Tá»•ng há»£p káº¿t quáº£ vÃ  káº¿t luáº­n

---

## ğŸ“ˆ Káº¿t quáº£ chÃ­nh

### ğŸ† **Hiá»‡u suáº¥t theo dataset**

| Dataset | RF Accuracy | NB Accuracy | Winner | ChÃªnh lá»‡ch |
|---------|-------------|-------------|--------|------------|
| **Mushroom** | **100.0%** | 89.3% | RF | +10.7% |
| **CreditCard** | **96.0%** | 93.6% | RF | +2.4% |
| **CreditCard_PCA** | **96.3%** | 93.9% | RF | +2.4% |
| **Adult** | **86.2%** | 80.6% | RF | +5.6% |
| **Adult_PCA** | **83.8%** | 78.5% | RF | +5.3% |
| **Sonar** | **83.3%** | 61.9% | RF | +21.4% |
| **Diabetes** | **82.0%** | 68.0% | RF | +14.0% |
| **Wine_PCA** | **79.7%** | 72.8% | RF | +6.9% |
| **Wine** | **79.1%** | 72.5% | RF | +6.6% |

### ğŸ“Š **Tá»•ng káº¿t**

- **Random Forest**: 87.37% (average)
- **Naive Bayes**: 79.01% (average)
- **Win Rate**: RF tháº¯ng 9/9 datasets (100%)

---

## ğŸ’¡ Káº¿t luáº­n

### âœ… **Khi nÃ o dÃ¹ng Random Forest?**
- âœ¨ Dá»¯ liá»‡u numeric vá»›i relationships phá»©c táº¡p
- âœ¨ Mixed data types (numeric + categorical)
- âœ¨ Cáº§n xá»­ lÃ½ outliers vÃ  missing values
- âœ¨ Features cÃ³ correlation cao
- âœ¨ Cáº§n feature importance
- âœ¨ Dataset lá»›n, cáº§n accuracy cao

### âœ… **Khi nÃ o dÃ¹ng Naive Bayes?**
- âœ¨ Text classification (TF-IDF)
- âœ¨ Categorical data vá»›i features Ä‘á»™c láº­p
- âœ¨ Cáº§n tá»‘c Ä‘á»™ training nhanh
- âœ¨ Dataset nhá»
- âœ¨ Real-time prediction
- âœ¨ Interpretability quan trá»ng

---

## ğŸ“ Ká»¹ thuáº­t Ä‘Ã£ Ã¡p dá»¥ng

### Preprocessing:
- [x] **Imputation**: Median/Mode strategy
- [x] **Scaling**: StandardScaler, RobustScaler
- [x] **Feature Engineering**: Interaction features
- [x] **Encoding**: Label Encoding cho categorical

### Data Handling:
- [x] **SMOTE**: Synthetic oversampling (Diabetes)
- [x] **Undersampling**: Random undersampling (CreditCard)
- [x] **PCA**: Dimensionality reduction (Sonar: 60â†’30)
- [x] **Feature Selection**: Mutual Information, SelectKBest

### Evaluation:
- [x] **Multiple metrics**: Accuracy, Precision, Recall, F1, AUC
- [x] **Confusion Matrix**: PhÃ¢n tÃ­ch chi tiáº¿t errors
- [x] **Cross-validation**: 5-fold CV
- [x] **Stratified split**: Giá»¯ phÃ¢n bá»‘ classes

---

## ğŸ› ï¸ YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8+
- **RAM**: Tá»‘i thiá»ƒu 8GB (vÃ¬ CreditCard dataset lá»›n)
- **Disk**: ~500MB cho datasets
- **OS**: Windows/Linux/MacOS

---

## ğŸ“š Code Features

### âœ¨ **Highlights:**

1. **Comment Ä‘áº§y Ä‘á»§ báº±ng tiáº¿ng Viá»‡t**
   - Giáº£i thÃ­ch tá»«ng bÆ°á»›c chi tiáº¿t
   - CÃ³ cÃ´ng thá»©c toÃ¡n há»c
   - VÃ­ dá»¥ cá»¥ thá»ƒ

2. **Preprocessing thÃ´ng minh**
   - Medical impossibility handling (Diabetes)
   - Smart imputation strategies
   - Appropriate scaling cho tá»«ng dataset

3. **Visualization cháº¥t lÆ°á»£ng cao**
   - Multiple resolutions (150 DPI, 300 DPI)
   - Professional charts
   - Easy to interpret

4. **Comprehensive evaluation**
   - 5 metrics per model
   - Confusion matrices
   - Win/loss analysis

---

## â“ Troubleshooting

### Lá»—i: "No such file or directory: 'datasets/...'"
â†’ Báº¡n chÆ°a download datasets. Xem má»¥c **ğŸ“¥ Download Datasets** á»Ÿ trÃªn

### Lá»—i: "Module 'imblearn' not found"
```bash
pip install imbalanced-learn
```

### Warning vá» sklearn features
â†’ CÃ³ thá»ƒ ignore, khÃ´ng áº£nh hÆ°á»Ÿng káº¿t quáº£

### Memory Error vá»›i CreditCard dataset
â†’ CreditCard dataset sáº½ tá»± Ä‘á»™ng undersampling. Náº¿u váº«n lá»—i, cÃ³ thá»ƒ comment out dÃ²ng 87-91 trong code

---

## ğŸ“§ TÃ¡c giáº£

**Sinh viÃªn**: [TÃªn cá»§a báº¡n]  
**TrÆ°á»ng**: Äáº¡i há»c Thá»§y Lá»£i  
**MÃ´n**: Khai phÃ¡ dá»¯ liá»‡u  
**NÄƒm**: 2025

---

## ğŸ“ Ghi chÃº quan trá»ng

âš ï¸ **Datasets khÃ´ng Ä‘Æ°á»£c push lÃªn GitHub** vÃ¬:
- File `creditcard.csv` lÃ  143.84 MB (vÆ°á»£t giá»›i háº¡n 100 MB cá»§a GitHub)
- TuÃ¢n thá»§ license cá»§a Kaggle/UCI

âš ï¸ **Khi clone repo:**
1. Clone project: `git clone [repo-url]`
2. Táº¡o thÆ° má»¥c: `mkdir datasets`
3. Download 6 datasets tá»« links trÃªn
4. Cháº¡y: `python random_forest_experiment.py`

---

## ğŸ¯ Quick Start

```bash
# 1. Clone repository
git clone https://github.com/chuanghiduoc/data-mining-cse-tlu.git
cd btl

# 2. Install dependencies
pip install -r requirements.txt

# 3. Download datasets (see links above)
# Äáº·t vÃ o thÆ° má»¥c datasets/

# 4. Run experiment
python random_forest_experiment.py

# 5. Xem káº¿t quáº£
# - eda_analysis_all.png
# - overall_comparison.png
# - confusion_matrices_all.png
```

---

## ğŸ“„ License

Dá»± Ã¡n nÃ y dÃ nh cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.

Datasets thuá»™c quyá»n sá»Ÿ há»¯u cá»§a:
- UCI Machine Learning Repository
- Kaggle Contributors

---

**â­ Náº¿u tháº¥y há»¯u Ã­ch, hÃ£y star repo nÃ y!**
