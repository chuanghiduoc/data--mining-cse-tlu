#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VÃ Dá»¤ MINH Há»ŒA RANDOM FOREST: Dá»° ÄOÃN HIá»†U SUáº¤T Há»ŒC Táº¬P SINH VIÃŠN
VÃ­ dá»¥ tá»± táº¡o, khÃ´ng cÃ³ trÃªn máº¡ng
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import warnings

# Táº¯t warnings khÃ´ng cáº§n thiáº¿t
warnings.filterwarnings('ignore', category=UserWarning)

# Táº¡o dataset vÃ­ dá»¥: Dá»± Ä‘oÃ¡n sinh viÃªn Ä‘áº­u/rá»›t mÃ´n Khai phÃ¡ dá»¯ liá»‡u
data = {
    'ID': range(1, 11),
    'Gio_hoc': [20, 15, 25, 12, 30, 18, 10, 22, 28, 16],
    'Bai_tap': [8, 6, 10, 4, 9, 7, 3, 8, 10, 5],
    'Diem_danh': [85, 90, 95, 70, 88, 92, 65, 80, 96, 75],
    'Kinh_nghiem_lap_trinh': [1, 0, 1, 0, 1, 1, 0, 0, 1, 0],  # 1=CÃ³, 0=KhÃ´ng
    'Ket_qua': [1, 0, 1, 0, 1, 1, 0, 0, 1, 0]  # 1=Äáº­u, 0=Rá»›t
}

df = pd.DataFrame(data)

print("="*60)
print("VÃ Dá»¤ MINH Há»ŒA RANDOM FOREST")
print("BÃ i toÃ¡n: Dá»± Ä‘oÃ¡n sinh viÃªn Ä‘áº­u/rá»›t mÃ´n Khai phÃ¡ dá»¯ liá»‡u")
print("="*60)

print("\nðŸ“Š Dá»® LIá»†U Gá»C:")
print(df[['Gio_hoc', 'Bai_tap', 'Diem_danh', 'Kinh_nghiem_lap_trinh', 'Ket_qua']])

# Chuáº©n bá»‹ dá»¯ liá»‡u
X = df[['Gio_hoc', 'Bai_tap', 'Diem_danh', 'Kinh_nghiem_lap_trinh']]
y = df['Ket_qua']

print(f"\nðŸ“ˆ PHÃ‚N Bá» Káº¾T QUáº¢:")
print(f"Äáº­u: {sum(y)} sinh viÃªn ({sum(y)/len(y)*100:.1f}%)")
print(f"Rá»›t: {len(y)-sum(y)} sinh viÃªn ({(len(y)-sum(y))/len(y)*100:.1f}%)")

# Táº¡o Random Forest vá»›i 3 cÃ¢y Ä‘á»ƒ minh há»a
print("\nðŸŒ² XÃ‚Y Dá»°NG RANDOM FOREST (3 cÃ¢y):")
print("-" * 40)

rf = RandomForestClassifier(
    n_estimators=3,
    max_features=2,  # Chá»‰ xem xÃ©t 2 features má»—i split
    max_depth=3,
    random_state=42,
    bootstrap=True
)

rf.fit(X, y)

# Hiá»ƒn thá»‹ thÃ´ng tin tá»«ng cÃ¢y
for i, tree in enumerate(rf.estimators_):
    print(f"\nðŸŒ³ CÃ‚Y {i+1}:")
    print(f"Features Ä‘Æ°á»£c chá»n: {tree.feature_importances_}")
    
    # In cáº¥u trÃºc cÃ¢y (Ä‘Æ¡n giáº£n)
    tree_rules = export_text(tree, feature_names=X.columns.tolist(), max_depth=2)
    print("Cáº¥u trÃºc cÃ¢y (2 level Ä‘áº§u):")
    print(tree_rules[:300] + "..." if len(tree_rules) > 300 else tree_rules)

# Dá»± Ä‘oÃ¡n cho sinh viÃªn má»›i
print("\nðŸŽ¯ Dá»° ÄOÃN CHO SINH VIÃŠN Má»šI:")
print("-" * 40)

# Sinh viÃªn X: 24 giá» há»c, 8 bÃ i táº­p, 87% Ä‘iá»ƒm danh, cÃ³ kinh nghiá»‡m
sinh_vien_moi_data = [[24, 8, 87, 1]]
# Táº¡o DataFrame vá»›i feature names Ä‘á»ƒ trÃ¡nh warning
sinh_vien_moi = pd.DataFrame(sinh_vien_moi_data, columns=X.columns)

# Dá»± Ä‘oÃ¡n tá»«ng cÃ¢y
print("Dá»± Ä‘oÃ¡n tá»«ng cÃ¢y:")
for i, tree in enumerate(rf.estimators_):
    pred = tree.predict(sinh_vien_moi)[0]
    proba = tree.predict_proba(sinh_vien_moi)[0][1]  # Probability cá»§a class 1 (Äáº­u)
    result = "Äáº­u" if pred == 1 else "Rá»›t"
    print(f"  CÃ¢y {i+1}: {result} (P(Äáº­u) = {proba:.3f})")

# Káº¿t quáº£ ensemble
final_pred = rf.predict(sinh_vien_moi)[0]
final_proba = rf.predict_proba(sinh_vien_moi)[0][1]
final_result = "Äáº­u" if final_pred == 1 else "Rá»›t"

print(f"\nðŸ† Káº¾T QUáº¢ CUá»I CÃ™NG (Ensemble):")
print(f"   Dá»± Ä‘oÃ¡n: {final_result}")
print(f"   XÃ¡c suáº¥t Ä‘áº­u: {final_proba:.3f} ({final_proba*100:.1f}%)")
print(f"   Äá»™ tin cáº­y: {'Cao' if final_proba > 0.7 or final_proba < 0.3 else 'Trung bÃ¬nh'}")

# Feature importance
print(f"\nðŸ“Š Má»¨C Äá»˜ QUAN TRá»ŒNG Cá»¦A CÃC Yáº¾U Tá»:")
print("-" * 45)
feature_importance = rf.feature_importances_
feature_names = X.columns.tolist()

for i, (name, importance) in enumerate(zip(feature_names, feature_importance)):
    print(f"{i+1}. {name}: {importance:.3f} ({importance*100:.1f}%)")

# TÃ­nh toÃ¡n Information Gain thá»§ cÃ´ng cho minh há»a
print(f"\nðŸ§® TÃNH TOÃN CHI TIáº¾T INFORMATION GAIN:")
print("-" * 50)

def calculate_entropy(labels):
    """TÃ­nh entropy"""
    unique_labels, counts = np.unique(labels, return_counts=True)
    entropy = 0
    for count in counts:
        p = count / len(labels)
        if p > 0:
            entropy -= p * np.log2(p)
    return entropy

def calculate_information_gain(X_col, y, threshold=None):
    """TÃ­nh Information Gain cho má»™t feature"""
    if threshold is None:
        threshold = np.median(X_col)
    
    # Entropy ban Ä‘áº§u
    initial_entropy = calculate_entropy(y)
    
    # Split data
    left_mask = X_col <= threshold
    right_mask = X_col > threshold
    
    if sum(left_mask) == 0 or sum(right_mask) == 0:
        return 0
    
    # Weighted entropy sau split
    left_entropy = calculate_entropy(y[left_mask])
    right_entropy = calculate_entropy(y[right_mask])
    
    weighted_entropy = (sum(left_mask)/len(y)) * left_entropy + (sum(right_mask)/len(y)) * right_entropy
    
    return initial_entropy - weighted_entropy

# TÃ­nh IG cho tá»«ng feature
print("Information Gain cho tá»«ng feature:")
for col in X.columns:
    if col == 'Kinh_nghiem_lap_trinh':
        # Binary feature
        ig = calculate_information_gain(X[col], y, threshold=0.5)
    else:
        ig = calculate_information_gain(X[col], y)
    print(f"  {col}: {ig:.4f}")

# Visualization
plt.figure(figsize=(15, 10))

# 1. Feature Importance
plt.subplot(2, 3, 1)
bars = plt.bar(feature_names, feature_importance, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
plt.title('Má»©c Ä‘á»™ quan trá»ng cÃ¡c yáº¿u tá»‘')
plt.ylabel('Importance')
plt.xticks(rotation=45, ha='right')
for bar, imp in zip(bars, feature_importance):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{imp:.3f}', ha='center', va='bottom')

# 2. PhÃ¢n bá»‘ giá» há»c theo káº¿t quáº£
plt.subplot(2, 3, 2)
dau_gio = df[df['Ket_qua']==1]['Gio_hoc']
rot_gio = df[df['Ket_qua']==0]['Gio_hoc']
plt.hist([dau_gio, rot_gio], bins=5, alpha=0.7, label=['Äáº­u', 'Rá»›t'], color=['green', 'red'])
plt.title('PhÃ¢n bá»‘ giá» há»c')
plt.xlabel('Giá» há»c')
plt.ylabel('Sá»‘ sinh viÃªn')
plt.legend()

# 3. PhÃ¢n bá»‘ bÃ i táº­p theo káº¿t quáº£
plt.subplot(2, 3, 3)
dau_bt = df[df['Ket_qua']==1]['Bai_tap']
rot_bt = df[df['Ket_qua']==0]['Bai_tap']
plt.hist([dau_bt, rot_bt], bins=5, alpha=0.7, label=['Äáº­u', 'Rá»›t'], color=['green', 'red'])
plt.title('PhÃ¢n bá»‘ sá»‘ bÃ i táº­p')
plt.xlabel('Sá»‘ bÃ i táº­p')
plt.ylabel('Sá»‘ sinh viÃªn')
plt.legend()

# 4. Tá»· lá»‡ Ä‘áº­u theo kinh nghiá»‡m láº­p trÃ¬nh
plt.subplot(2, 3, 4)
co_kn = df[df['Kinh_nghiem_lap_trinh']==1]['Ket_qua'].mean()
khong_kn = df[df['Kinh_nghiem_lap_trinh']==0]['Ket_qua'].mean()
plt.bar(['CÃ³ kinh nghiá»‡m', 'KhÃ´ng kinh nghiá»‡m'], [co_kn, khong_kn], 
        color=['darkgreen', 'darkred'], alpha=0.7)
plt.title('Tá»· lá»‡ Ä‘áº­u theo kinh nghiá»‡m')
plt.ylabel('Tá»· lá»‡ Ä‘áº­u')
for i, v in enumerate([co_kn, khong_kn]):
    plt.text(i, v + 0.02, f'{v:.1%}', ha='center', va='bottom')

# 5. Scatter plot: Giá» há»c vs BÃ i táº­p
plt.subplot(2, 3, 5)
colors = ['green' if x == 1 else 'red' for x in df['Ket_qua']]
plt.scatter(df['Gio_hoc'], df['Bai_tap'], c=colors, alpha=0.7, s=100)
plt.xlabel('Giá» há»c')
plt.ylabel('BÃ i táº­p')
plt.title('Giá» há»c vs BÃ i táº­p')
# ThÃªm Ä‘iá»ƒm sinh viÃªn má»›i
plt.scatter(24, 8, c='blue', s=200, marker='*', label='Sinh viÃªn má»›i')
plt.legend()

# 6. Prediction confidence
plt.subplot(2, 3, 6)
# Táº¡o dá»¯ liá»‡u giáº£ cho visualization
test_students = []
predictions = []
confidences = []

for gio in range(10, 35, 5):
    for bt in range(3, 11, 2):
        for kn in [0, 1]:
            test_point_data = [[gio, bt, 85, kn]]  # Äiá»ƒm danh cá»‘ Ä‘á»‹nh 85%
            test_point_df = pd.DataFrame(test_point_data, columns=X.columns)
            pred_proba = rf.predict_proba(test_point_df)[0][1]
            test_students.append(test_point_data[0])
            predictions.append(pred_proba > 0.5)
            confidences.append(pred_proba)

# Váº½ decision boundary (simplified)
x_range = range(10, 35)
y_range = range(3, 11)
confidence_colors = ['red' if c < 0.3 else 'yellow' if c < 0.7 else 'green' for c in confidences]

plt.title('Äá»™ tin cáº­y dá»± Ä‘oÃ¡n')
plt.xlabel('Máº«u test')
plt.ylabel('Confidence')
plt.bar(range(len(confidences)), confidences, color=confidence_colors, alpha=0.7)
plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.savefig('random_forest_demo.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nðŸ’¡ NHáº¬N XÃ‰T:")
print("="*40)
print("âœ… Random Forest káº¿t há»£p 3 cÃ¢y vá»›i cÃ¡c gÃ³c nhÃ¬n khÃ¡c nhau")
print("âœ… Má»—i cÃ¢y chá»‰ xem xÃ©t 2/4 features â†’ tÄƒng tÃ­nh Ä‘a dáº¡ng")
print("âœ… Káº¿t quáº£ ensemble tin cáº­y hÆ¡n tá»«ng cÃ¢y riÃªng láº»")
print("âœ… Feature importance cho tháº¥y 'Kinh nghiá»‡m láº­p trÃ¬nh' quan trá»ng nháº¥t")
print("âœ… Probability output giÃºp Ä‘Ã¡nh giÃ¡ Ä‘á»™ tin cáº­y cá»§a dá»± Ä‘oÃ¡n")

print(f"\nðŸŽ“ Káº¾T LUáº¬N:")
print("="*30)
print(f"Sinh viÃªn vá»›i profile [24h há»c, 8 bÃ i táº­p, 87% Ä‘iá»ƒm danh, cÃ³ kinh nghiá»‡m]")
print(f"cÃ³ {final_proba*100:.1f}% kháº£ nÄƒng Äáº¬U mÃ´n Khai phÃ¡ dá»¯ liá»‡u")
print("ÄÃ¢y lÃ  vÃ­ dá»¥ minh há»a thuáº­t toÃ¡n Random Forest hoÃ n toÃ n tá»± táº¡o!")
